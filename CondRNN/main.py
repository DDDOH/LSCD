# %%
import os
import datetime
import numpy as np
import matplotlib.pyplot as plt
# from geomloss import SamplesLoss
import torch
from torch.utils.tensorboard import SummaryWriter


from lscd import utils
from lscd import dataset
from lscd import models
from lscd import metric

# TODO
"""
Dataset
    GMM                 [o]
    PGnorta             [o]
    Bikeshare           [ ]
    other dataset       [ ]
Baseline
    KDE                 [ ]
    GMM                 [ ]
ML model
    RNN             
    NoisyReinforcedMLP
    MLP
Evaluation metric
    Run through queue
    pairwise plot
    marginal mean                       [o]
    marginal variance                   [o]
    Pierre correlation                  [o]
    Gaussian approximated W-distance    [o]
Others
    Gaussian gradient approximation precision (when lambda is small)
"""
# See how python import self defined modules: https://realpython.com/python-import/

# TODO Add KDE for comparision.
# TODO Finalize visulization code.
# TODO Make MLP present acceptable results.

# Get synthetic training set.
COND_LEN = 10
N_SAMPLE = 2000

DATA_NAME = 'PGnorta'  # 'PGnorta' or 'multivariate_normal'
MODEL_NAME = 'BaselineGMM'  # 'CondLSTM' or 'CondMLP' or 'BaselineGMM
if DATA_NAME == 'multivariate_normal':
    SEQ_LEN = 50
    data = dataset.multivariate_normal.MultivariateNormal(seq_len=SEQ_LEN)
if DATA_NAME == 'PGnorta':
    data = dataset.pg_norta.get_PGnorata_from_img()
    seq_len = data.seq_len
training_set = data.sample(n_sample=N_SAMPLE)

dirname = os.path.dirname(__file__)
date = datetime.datetime.now().strftime("%d-%m-%y_%H:%M")
result_time_dir = os.path.join(dirname, 'results', date)
result_dir = os.path.join(dirname, 'results')
if not os.path.exists(result_dir):
    os.mkdir(result_dir)
if not os.path.exists(result_time_dir):
    os.mkdir(result_time_dir)
writer = SummaryWriter(result_time_dir)

# pre processing


class Scaler():
    def __init__(self, data):
        """Initialize the Scaler class.

        Args:
            data (np.array): 2d numpy array. Each row represents one sequence.
                             Each column is one timestep.
                             Only works for sequence with one feature each time step, temporarily.
        """
        self.data = data
        self.mean = np.mean(data, axis=0)
        self.std = np.std(data, axis=0)

    def transform(self, data=None):
        """Transform data to standard normal distribution.

        Transform data for each time stamp to an individual standard normal distribution.

        Args:
            data (np.array, optional): If not specified, default to the dataset used to initialize this Scaler.
                                       Defaults to None.

        Returns:
            np.array: The transformed data.
        """
        if not data:
            data = self.data
        return (data - self.mean) / self.std

    def inverse_transform(self, data):
        return data * self.std + self.mean


scale = False
if scale:
    scaler = Scaler(training_set)
    ori_training_set = training_set
    training_set = scaler.transform()

training_set = torch.Tensor(training_set).unsqueeze(-1)
conditions = training_set[:, :COND_LEN, :]
dependents = training_set[:, COND_LEN:, :]

# baseline methods
if MODEL_NAME == 'BaselineGMM':
    X_q = conditions.squeeze(-1)[0, :]
    from sklearn.mixture import GaussianMixture
    gmm_joint = GaussianMixture(n_components=10, random_state=0).fit(
        training_set.squeeze(-1))
    gmm_cond = models.gmm.get_cond_gm(
        gmm_joint, x_q=X_q)

    gmm_cond_samples, _ = gmm_cond.sample(n_samples=1000)
    real_cond_samples = data.sample_cond(X_1q=X_q, n_sample=200)

    metric.classical.evaluate(
        real_cond_samples=real_cond_samples, fake_cond_samples=gmm_cond_samples, dir_filename='test_gmm.jpg')

if MODEL_NAME == 'BaselineKDE':

    # TODO: Poisson count simulator layer


def train_iter(model, loss_func, lr=0.0002, epochs=1000):
    """Train the model with given loss.

    Args:
        model ([type]): [description]
        loss_func ([type]): Loss function. The first input is sequence generated by model, second input is target.
        lr (float, optional): [description]. Defaults to 0.0002.
        epochs (int, optional): [description]. Defaults to 1000.
    Returns:
        model (torch.Tensor): Trained model.
    """

    optimizer = torch.optim.Adam(predictor.parameters(), lr)

    # loss_curve = []
    if isinstance(model, models.model.CondLSTM):
        h_records = []
        c_records = []
        target = training_set[:, 1:]
    if isinstance(model, models.model.CondMLP):
        target = training_set[:, COND_LEN:]

    for i in range(epochs):
        predictor.zero_grad()
        pred_value = models.model.predict_MLP(predictor, training_set)
        # TODO pred_value = predict_full_RNN(training_set, predictor)
        loss = loss_func(pred_value, target)
        loss.backward()
        print('[%d/%d] Loss: %.4f' % (i, epochs, loss.item()))
        # loss_curve.append(loss.detach())
        optimizer.step()

        if model == 'CondLSTM':
            h_records.append(predictor.h_0.clone().detach().squeeze())
            c_records.append(predictor.c_0.clone().detach().squeeze())

        if i % 10 == 0:
            # Plot predicted value vs target value.
            plt.figure(figsize=(10, 5))
            plt.subplot(121)
            plt.plot(pred_value.squeeze(-1).detach().T, c='r', alpha=0.02)
            plt.plot(target.squeeze(-1).T, c='g', alpha=0.02)
            plt.subplot(122)
            condition = training_set[:, :COND_LEN]
            true_dependent = training_set[:, COND_LEN:]

            # TODO: modify old only LSTM code
            # TODO pred_on_condition = predict_condition(condition, predictor)
            # TODO plt.plot(pred_on_condition.squeeze(-1).detach().T,
            #  c='r', alpha=0.02)
            fake_dependent = models.model.predict_MLP(
                predictor, training_set).detach()

            plt.plot(fake_dependent.squeeze(-1).T, c='r', alpha=0.1)
            plt.plot(true_dependent.squeeze(-1).T, c='g', alpha=0.1)
            filename = os.path.join(result_dir, 'prediction_step_%d.jpg' % (i))
            plt.savefig(filename)
            plt.close()

            if isinstance(model, models.model.CondLSTM):
                plt.figure(figsize=(20, 5))
                # plt.pcolormesh(np.stack(h_records, axis=0))
                plt.subplot(121)
                plt.plot(np.stack(h_records, axis=0))
                plt.subplot(122)
                plt.plot(np.stack(c_records, axis=0))
                filename = os.path.join(result_dir, 'h_c_records.jpg')
                plt.savefig(filename)
                plt.close()

            fake_statistic = utils.plot_mean_var_cov(
                fake_dependent.squeeze(-1), dpi=45)
            writer.add_image("fake", fake_statistic, i)

        writer.add_scalar('Loss curve',
                          loss.item(), i)

        writer.flush()

    return model


if MODEL_NAME == 'CondLSTM':
    # Config for CondLSTM
    noise_dim = 2
    hidden_dim = 256
    n_feature = 1
    # may also use embedded time step as input feature
    in_feature = noise_dim + n_feature
    n_layers = 1
    out_feature = n_feature
    # since the number of data in training set is small, use the whole training set in each iteration, temporarily
    batch_size = N_SAMPLE
    predictor = models.model.CondLSTM(
        in_feature=in_feature, out_feature=out_feature)
if MODEL_NAME == 'CondMLP':
    predictor = models.model.CondMLP(seed_dim=seq_len-COND_LEN,
                                     COND_LEN=COND_LEN, seq_len=seq_len, hidden_dim=256)


# summary(predictor, [(COND_LEN, n_feature), (COND_LEN, noise_dim), (2, 1, hidden_dim)])

# train the model to minimize MSE
# loss_func = loss = nn.MSELoss()
# train_iter(predictor, loss_func, lr=0.0005)

# %%
# Training to minimize utils.w_distance seems quite unstable and fail to converge
# even if pretraining to minimize MSE
# reason unknown

# train_iter(predictor, loss_func=nn.MSELoss(), lr=0.0005, epochs=300)

# def weighted_dist(generated, target):
#     return utils.w_distance(generated.squeeze(-1), target.squeeze(-1))

# train_iter(predictor, loss_func=weighted_dist, lr=0.0005)

# %%

# sinkorn_loss = SamplesLoss("sinkhorn", p=2, blur=0.05, scaling=0.8)


# def sinkhorn_dist(generated, target):
#     return sinkorn_loss(generated.squeeze(-1), target.squeeze(-1))


# train_iter(predictor, loss_func=sinkhorn_dist, lr=0.005, epochs=300)
